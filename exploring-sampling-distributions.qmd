---
title: "Exploring sampling distribution concepts"
format: 
  dashboard:
    theme:
      - spacelab
      - custom.scss
server: shiny
---

```{r setup, include = FALSE}
library(tidyverse)
library(latex2exp)
library(shinydashboard)
```

```{r}
#| context: server

source('global.R', local = FALSE)

################################################################################
### REACTIVE OBJECTS
################################################################################

samples <- reactive({
  generate_samples(bowl, size=input$sample_size, reps = input$replications) |> 
    summarise(
      prop_red = sum(colour == "Red")/n()
    )
})

dist_pdf_data <- reactive({
  generate_dist_pdf(input$distributions)
})

sampling_dist_data <- reactive({
  generate_sample_dist(input$distributions, size=input$dist_sample_size, reps=input$dist_replications) |> 
    summarise(
      sample_mean = mean(x)
    )
})

boot_dist_data <- reactive({
  generate_boot_dist(salt_samples, reps=input$boot_replications) |> 
    summarise(
      mean_ppm = mean(na_ppm)
    )
})

################################################################################
### RENDER FUNCTIONS
################################################################################
output$propDistPlot <- renderPlot({
  
  samples() |> 
    ggplot(aes(x=prop_red)) +
    geom_histogram(fill="steelblue", colour="black", boundary=0.4, binwidth=0.05, alpha=0.7) +
    labs(
      x="Sample proportion of red balls",
      y="Count",
      title = "The distribution of sample proportions",
      subtitle = "How does the shape of the distribution change as the sample size and number of samples changes?"
    )
  
})

output$distPdfPlot <- renderPlot({
  
  dist_name <- names(which(distribution_options == input$distributions))
  
  dist_pdf_data() |> 
    ggplot(aes(x=x, y=y)) +
    geom_line(linewidth=1) +
    labs(
      x="x",
      y="Likelihood",
      title = str_glue("The {dist_name} distribution")
    )
  
})

output$sampleDistPlot <- renderPlot({
  dist_name <- names(which(distribution_options == input$distributions))
  sampling_dist_data() |> 
    ggplot(aes(x=sample_mean)) +
    geom_histogram(fill="steelblue", colour="black", alpha=0.7, bins=30) +
    labs(
      x="Sample mean",
      y="Count",
      title = str_glue("The distribution of sample means for {dist_name}"),
      subtitle = "How does the shape of the distribution change as the sample size and number of samples changes?"
    )
})

output$bootDistPlot <- renderPlot({

  # add confidence inverval and the sample mean to the plot in addition to the
  # bootstrap distribution itself.
  sample_mean <- round(mean(salt_samples$na_ppm), 2)
  std_err <- sd(boot_dist_data()$mean_ppm)
  dist_mean <- round(mean(boot_dist_data()$mean_ppm), 2)
  ci_level <- as.numeric(input$ci_level)
  ci_lower <- dist_mean - ci_level*std_err
  ci_upper <- dist_mean + ci_level*std_err
  ci_label <- names(which(ci_levels == input$ci_level))
  
  plot <- boot_dist_data() |> 
    ggplot(aes(x=mean_ppm)) +
    geom_histogram(fill="steelblue", colour="black", alpha=0.5, bins=30) +
    geom_vline(xintercept=sample_mean, linewidth=1, colour="firebrick") +
    geom_vline(xintercept=dist_mean, linewidth=1, linetype="dashed", colour="grey40") +
    geom_vline(xintercept=ci_lower, linewidth=1, colour="black") +
    geom_vline(xintercept=ci_upper, linewidth=1, colour="black") +
    annotate("rect", xmin=ci_lower, xmax=ci_upper, ymin=0, ymax=Inf, fill="grey80", alpha=0.4)
    
  y_anchor <- max(ggplot_build(plot)$data[[1]]$count)
    
  plot +
    annotate("text", x=sample_mean+0.05, y=y_anchor*0.9, size=5, 
             label=str_glue("Original sample mean: {sample_mean}"), colour="firebrick", hjust="left") +
    annotate("text", x=dist_mean-0.05, y=y_anchor*0.95, size=5, 
             label=str_glue("Distribution mean: {dist_mean}"), colour="black", hjust="right") +
    annotate("text", x=ci_lower-0.05, y=y_anchor/2,label="Lower end of confidence interval", angle=90, size=6) +
    annotate("text", x=ci_upper+0.05, y=y_anchor/2,label="Upper end of confidence interval", angle=270, size=6) +
    labs(
      x="Resample mean of salt concentration (ppm)",
      y="Count",
      title = str_glue("The bootstrap distribution of salt concentration with {input$boot_replications} resamples"),
      subtitle = str_glue("The {ci_label} confidence interval goes from {round(ci_lower, 2)} to {round(ci_upper, 2)}")
    )
})

output$meanBox <- renderValueBox({
  box(
    round(mean(samples()$prop_red),4)
    # title = "Distribution mean"
  )
})

output$sdBox <- renderValueBox({
  box(
    round(sd(samples()$prop_red), 4)
    # title = "Distribution standard error"
  )
})
```

# Introduction 

## {height="35%"}

::: {.card title="Introduction to sampling distributions"}
**What is a sampling distribution?** After exploring the scenario below, you should have a good idea of what a sampling distribution is and how it changes based on the sampling procedure employed. You should understand how the number of samples take and the size of the samples affects the estimate of the mean and standard error of the population parameter being studied. You will also be able to explain the basic idea behind the **Central Limit Theorem** and get some hands on experience with the concept of **bootstrapping**. All set? Let's get started!

We often don't know the value of the population parameter that we're trying to study. To better understand the value of the parameter, we can either take a **census** or take one or more **samples**. A census involves making an observation of every individual in the population. This is often either impossible or prohibitively time consuming and expensive. This is why sampling is typically used to make an estimate of the parameter. When sampling, we take one or more samples of a given size from the population. In practice, a single sample is often taken and the sample size is a large as time and budget will allow.
:::

## {height="65%"}

::: {.card title="Important terms"}

**Sample** - A set of observations made on a subset of the population of interest. Measuring the length of 50 trout pulled from a lake, polling potential voters about a policy issue, and inspecting products on an assembly line at random for defects are all examples of sampling.

**Biased & random samples** - When samples are taken, each individual in the population from which samples are taken should have an equal chance of being included in the sample. This results in a *random* sample that is representative of the population. Summary statistics computed from such a sample can be generalized to the larger population. If certain individuals or groups of individuals within the population are more likely to be sampled than others, then the sample is said to be *biased*. Estimates made on a biased sample are more likely to incorrect. As a result, estimates biased samples do not generalize to the population.

**Population parameter** - This is a numeric dimension of the population that we want to estimate a value for. We don't know the true value. If we did, we wouldn't need to estimate it! 

**Point estimate** - A summary statistic of a sample drawn from a population that estimates a population parameter of interest. It might be the mean height of college women basketball players or the proportion of voters who support a specific initiative.

**Sample statistic** - Another name for point estimate.

**Sampling variance** - When drawing samples from a population, there will be some variability in the individual observations. This results in variation in the sample statistics computed from the samples.

**Sampling distribution** - When multiple samples are taken and the summary statistic of interest is computed, *sampling variation* will result in a distribution of values for the sample statistic.

**Standard error** - The standard error is simply the standard deviation of the sampling distribution.

:::

# Sampling Distributions

##

::: {.card title="A basic sampling scenario"}
In this scenario, we want to know what proportion of balls in a bowl are red. The bowl contains 2400 balls, so taking a census would be really tedious. There are different shovels that can be used to take samples and many samples can be taken. Whether sampling 25, 50, 75, or 100 balls at a time, it is unlikely that any two samples will have the same proportion of red balls. This is a demonstration of *sampling variance*. The degree of sampling variance in turn has an effect on the sampling distribution.

In the panel below, you can explore how the sampling distribution changes as the sample size and number of samples is increased or decreased. Experiment with different sample sizes and number of samples taken. How does the distribution change? What about the sampling distribution mean and standard error? What is a reasonable estimate for the proportion of red balls in the bowl?
:::

## {height="50%"}

```{r}
#| content: card-sidebar
#| width: 25%

sliderInput("sample_size", "Sample size:", 
            min = 25, max = 100, value = 50, step=25, ticks=FALSE, width="95%")
selectInput("replications", "Number of samples taken:", 
            choices=replicate_count, selected=replicate_count[2], width="95%")
```


```{r}
#| title: Sampling distribution visualization
plotOutput("propDistPlot")
```

## {height="15%"}

### {width="50%"}

::: {.valuebox icon="align-bottom" color="blue" title="Distribution mean"}
```{r}
valueBoxOutput("meanBox")
```
:::
### {width="50%"}
::: {.valuebox icon="bar-chart-line" color="blue" title="Distribution standard error"}
```{r}
valueBoxOutput("sdBox")
```
:::

# The Central Limit Theorem

## {height="30%"}

::: {.card title="What is the Central Limit Theorem?"}
The Central Limit Theorem is the pillar that much of classical statistics rests on. This theorem states that the statistics computed from repeatedly sampling from a population will be approximate normally distributed, *regardless of the underlying distribution of the population parameter*. The greater the number of samples, the better this approximation. This is an important result because it allows us to use the normal distribution's symmetry to construct things like *confidence intervals* that help us quantify the uncertainty of our population parameter estimates.

The form below allows you to see this for yourself. Experiment with sample size, number of samples, and the nature of the underlying distribution to see the Central Limit Theorem in action. In particular, notice the very different shapes of the underlying distributions that the samples are drawn from.
:::

## {height="70%"}

```{r}
#| content: card-sidebar
#| width: 25%

selectInput("distributions", "Choose a distribution:", 
            choices=distribution_options, width="95%")
sliderInput("dist_sample_size", "Sample size:", 
            min = 25, max = 100, value = 50, step=25, ticks=FALSE, width="95%")
selectInput("dist_replications", "Number of samples taken:", 
            choices=replicate_count, selected=replicate_count[2], width="95%")
```

###

::: {.card title="The Central Limit Theorem in Action"}
```{r}
plotOutput("distPdfPlot")
plotOutput("sampleDistPlot")
```
:::

# Bootstrapping

## {height="30%"}

:::: {.card title="What Bootstrapping?"}
A mentioned earlier, we typically cannot take multiple samples. Usually, a single sample is taken and it is made as large as possible based on constraints like time and money. If we only have one sample, we don't know anything about the *sampling variance*. Bootstrapping, which is a form of *resampling with replacement*, allows us approximate the sampling distribution using a single sample. The basic idea of sampling with replacement is that we create new samples by drawing observations from the original sample and then returning them to the pool to be potentially drawn again. This is what is meant by the term *with replacement*. An observation can be drawn more than once. The new sample will be the same size as the original sample. Replacement is obviously critical as without it, we would just recreate the original sample over and over. By allowing observations to appear more than once, we are introducing *sampling variance*, which allows us to create a distribution. To distinguish this distribution from the *sampling distribution*, we call it a **bootstrap** distribution.

We can use the standard error of the bootstrap distribution to create a **confidence interval** that tells us how confident we are, given the data we have, that the population parameter is contained within the interval. For example, if we created 100 bootstrap distributions we would expect 95% of those distributions to contain the true value of the population parameter.

:::{.callout-caution title="Interpreting confidence intervals"}
The interpretation of confidence intervals in classical statistics is somewhat confusing. The typical way it is interpreted is that with a 95% confidence interval, there is a 95% chance that the given bootstrap or sampling distribution contains the true population parameter value. This is not correct. Instead, what this confidence interval indicates is that if we generated 100 distributions with the data we have, we would expect 95 of them to contain the true value of the population parameter.
:::

### The scenario
In an effort to measure the amount of salt running into Lake Macatawa, a water sample of 32 observations of salt concentration in parts per million was taken from different parts of the lake. From this sample, a bootstrap distribution can be constructed by resampling with replacement. To see how the bootstrap distribution changes, experiment with the form below. Notice how the width of the confidence interval changes with level of confidence chosen. What is the range for the 80% and 95% confidence intervals? What is a reasonable estimate for the mean salt concentration in parts per million (ppm)?
::::

## {height="70%"}

```{r}
#| content: card-sidebar
#| width: 25%

selectInput("boot_replications", "Number of resamples:", 
            choices=replicate_count, selected=replicate_count[2], width="95%")
selectInput("ci_level", "Select confidence level:", 
            choices=ci_levels, selected=ci_levels[3], width="95%")
```

::: {.card title="Bootstrap Distribution & Confidence Interval"}
```{r}
#| content: tabset
plotOutput("bootDistPlot")
```
:::